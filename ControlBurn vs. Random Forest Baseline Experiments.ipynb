{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Required Packages\n",
    "import sklearn\n",
    "import statistics\n",
    "import scipy as sc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import cvxpy as cp\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "import re\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "import mosek\n",
    "import statsmodels.api as sm\n",
    "import gc\n",
    "import json\n",
    "import pickle\n",
    "from pmlb import fetch_data\n",
    "from pmlb import classification_dataset_names, regression_dataset_names\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Control Burn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def converge_test(sequence, threshold,length):\n",
    "    diff = np.diff(sequence)\n",
    "    if len(diff) < (length+1):\n",
    "        return False\n",
    "    else:\n",
    "        return ( max(np.abs(diff[-length:])) < threshold)\n",
    "    \n",
    "def build_trees_bag(arg):\n",
    "    xTrain = arg[0]\n",
    "    yTrain = arg[1]\n",
    "    xTest= arg[2]\n",
    "    yTest= arg[3]\n",
    "    max_depth= arg[4]\n",
    "    problem_type = arg[5]\n",
    "    loss_type = arg[6]\n",
    "    lambd= arg[7]\n",
    "    threshold= arg[8] \n",
    "    sketch = arg[9]\n",
    "   \n",
    "    train = xTrain\n",
    "    train = train.reset_index().drop('index',axis = 1)\n",
    "    train['yTrain'] = list(yTrain)\n",
    "    features = xTrain.columns\n",
    "    nfeatures = len(features)\n",
    "    importance_key = pd.DataFrame(features,columns = ['Features'])\n",
    "    tree_results = []\n",
    "    i = 0\n",
    "    depth = 1\n",
    "    total_trees = 0\n",
    "    \n",
    "    for depth in range (1,max_depth+1):\n",
    "        i = 0\n",
    "        ### Early Stopping\n",
    "        early_stop_pred = []\n",
    "        early_stop_train_err = []\n",
    "        converged = False\n",
    "        \n",
    "        while converged == False:\n",
    "            train1 = train.sample(n = len(train), replace = True)\n",
    "        \n",
    "            yTrain1 = train1['yTrain']\n",
    "            xTrain1 = train1[features]\n",
    "            \n",
    "            if problem_type == 'Regression':\n",
    "                clf = DecisionTreeRegressor(max_depth = depth)\n",
    "            elif problem_type == 'Classification':\n",
    "                clf = DecisionTreeClassifier(max_depth = depth)\n",
    "        \n",
    "            clf.fit(xTrain1,yTrain1)\n",
    "            imp = pd.DataFrame(np.column_stack((xTrain1.columns,clf.feature_importances_)), columns = ['Features','Importances'])\n",
    "            used = imp[imp['Importances']>0]['Features'].values\n",
    "            feature_indicator = [int(x in used) for x in features]\n",
    "            \n",
    "            if problem_type == 'Regression':\n",
    "                pred = clf.predict(xTrain[features])\n",
    "                test_pred = clf.predict(xTest[features])\n",
    "            elif problem_type == 'Classification':\n",
    "                if loss_type == 'logistic':\n",
    "                    pred = clf.predict_proba(xTrain[features])[:,1]\n",
    "                    test_pred = clf.predict_proba(xTest[features])[:,1]\n",
    "                elif loss_type == 'hinge':\n",
    "                    pred = clf.predict(xTrain[features])\n",
    "                    test_pred = clf.predict(xTest[features])\n",
    "                \n",
    "            feature_importances = pd.merge(importance_key,imp, on = 'Features', how = 'left').fillna(0)['Importances'].values\n",
    "            tree_results.append([pred,feature_indicator,feature_importances, test_pred  ,clf,xTrain1,yTrain1,features])\n",
    "            i = i+1\n",
    "            total_trees = total_trees+1\n",
    "            early_stop_pred.append(pred)\n",
    "            early_stop_train_err.append(np.sqrt(np.mean((np.mean(early_stop_pred,axis = 0) - yTrain)**2)))\n",
    "            converged = converge_test(early_stop_train_err,threshold,5)\n",
    "            \n",
    "    return tree_results\n",
    "\n",
    "def solve_step_nonsketch(arg, tree_results):\n",
    "    xTrain = arg[0]\n",
    "    yTrain = arg[1]\n",
    "    xTest= arg[2]\n",
    "    yTest= arg[3]\n",
    "    max_depth= arg[4]\n",
    "    problem_type = arg[5]\n",
    "    loss_type = arg[6]\n",
    "    lambd= arg[7]\n",
    "    threshold= arg[8] \n",
    "    optimization_type = arg[9] #penalized or constrained\n",
    "    \n",
    "    feature_list = xTrain.columns\n",
    "    tree_pred = np.transpose(np.array([np.array(row[0]) for row in tree_results]))\n",
    "    test_pred = np.transpose(np.array([np.array(row[3]) for row in tree_results]))\n",
    "    indicators = np.transpose(np.array([np.array(row[1]) for row in tree_results]))\n",
    "    w = cp.Variable(len(tree_results),nonneg=True)\n",
    "    \n",
    "    if optimization_type == 'penalized':\n",
    "        constraints = []\n",
    "        if problem_type == 'Regression':\n",
    "            loss = cp.sum_squares(cp.matmul(tree_pred,w)-yTrain) \n",
    "            objective = (1/len(yTrain))*loss + lambd*cp.norm(cp.matmul(indicators,w),1)\n",
    "        elif problem_type == 'Classification':\n",
    "            if loss_type == 'logistic':\n",
    "                loss = -cp.sum(cp.multiply(yTrain, tree_pred@ w) - cp.logistic(tree_pred @ w))\n",
    "                objective = (1/len(yTrain))*loss + lambd*cp.norm(cp.matmul(indicators,w),1)\n",
    "            elif loss_type == 'hinge':\n",
    "                loss =  cp.sum(cp.pos(1 - cp.multiply(yTrain, tree_pred @ w)))\n",
    "                objective =  (1/len(yTrain))*loss + lambd*cp.norm(cp.matmul(indicators,w),1)\n",
    "  \n",
    "        \n",
    "    if optimization_type == 'constrained':\n",
    "        if problem_type == 'Regression':\n",
    "            objective = cp.sum_squares(cp.matmul(tree_pred,w)-yTrain) \n",
    "        elif problem_type == 'Classification': \n",
    "            if loss_type == 'logistic':\n",
    "                objective = -cp.sum(cp.multiply(yTrain, tree_pred@ w) - cp.logistic(tree_pred @ w))\n",
    "            elif loss_type == 'hinge':\n",
    "                objective = cp.sum(cp.pos(1 - cp.multiply(yTrain, tree_pred @ w)))\n",
    "        constraints = [cp.norm(cp.matmul(indicators,w),1)<= lambd]\n",
    "    \n",
    "    prob = cp.Problem(cp.Minimize(objective),constraints)\n",
    "    prob.solve(solver = cp.MOSEK,mosek_params = {mosek.dparam.optimizer_max_time: 10000.0} )\n",
    "    weights = np.asarray(w.value)\n",
    "    low_values_flags = np.abs(weights) < 10**-3  \n",
    "    weights[low_values_flags] = 0 \n",
    "    tree_ind = np.where(weights >0)[0]\n",
    "    \n",
    "    if len(tree_ind)==0:\n",
    "        if problem_type == 'Regression':\n",
    "            return([[],np.sqrt(np.mean((yTest )**2)),0,np.sqrt(np.mean((yTrain )**2))])\n",
    "        else:\n",
    "            return([[],.5,0,.5])\n",
    "    \n",
    "    importances = np.array([np.array(row[2]) for row in tree_results])\n",
    "    feature_importances = np.mean(importances[tree_ind],axis = 0)\n",
    "    nonzero_features = xTrain.columns[np.where(feature_importances >0)[0]]\n",
    "    \n",
    "    if problem_type == 'Regression':\n",
    "        rf = RandomForestRegressor(n_estimators = 100).fit(xTrain[nonzero_features],yTrain)\n",
    "        test_pred = rf.predict(xTest[nonzero_features])\n",
    "        train_pred = rf.predict(xTrain[nonzero_features])\n",
    "        train_error =  np.sqrt(np.mean((yTrain -train_pred)**2))\n",
    "        test_error = np.sqrt(np.mean((yTest -test_pred)**2))\n",
    "        return([feature_importances,test_error,len(nonzero_features),train_error])\n",
    "        \n",
    "        \n",
    "    elif problem_type == 'Classification' :\n",
    "        rf = RandomForestClassifier(n_estimators = 100).fit(xTrain[nonzero_features],yTrain)\n",
    "        test_pred = rf.predict_proba(xTest[nonzero_features])[:,1]\n",
    "        train_pred = rf.predict_proba(xTrain[nonzero_features])[:,1]\n",
    "        train_error = sklearn.metrics.roc_auc_score(yTrain,train_pred)\n",
    "        test_error = sklearn.metrics.roc_auc_score(yTest,test_pred)\n",
    "        return([feature_importances,test_error,len(nonzero_features),train_error])\n",
    "    \n",
    "def run_experiment(arg,ntrials,features_to_find,search_limit,l_start,method):\n",
    "    test_error_result = []\n",
    "    nonzero_result = []\n",
    "    train_error_result = []\n",
    "    trial = 0\n",
    "    while trial < ntrials:\n",
    "        \n",
    "        #Build Trees\n",
    "        tree_results = method(arg)\n",
    "        LL = 0\n",
    "        RL = l_start\n",
    "        to_find = 0\n",
    "        counter1 = 0\n",
    "        count_array = []\n",
    "        while to_find <= features_to_find:\n",
    "            arg_list = []\n",
    "            lambd = (LL + RL)/2\n",
    "               \n",
    "            arg[7] = lambd\n",
    "            result = solve_step_nonsketch(arg,tree_results)\n",
    "            test_acc = result[1]\n",
    "            nonzero = result[2]\n",
    "            train_acc = result[3]\n",
    "            print(nonzero,to_find,lambd)\n",
    "            #Append Results\n",
    "            test_error_result.append(test_acc)\n",
    "            nonzero_result.append(nonzero)\n",
    "            train_error_result.append(train_acc)\n",
    "            count_array.append(nonzero)\n",
    "        \n",
    "            freq = pd.DataFrame(np.column_stack(np.unique(count_array, return_counts = True)),columns = ['value','counts'])\n",
    "            count_to_find = freq.loc[freq['value']==to_find]['counts'].values\n",
    "            \n",
    "            \n",
    "            if arg[9] != 'constrained':\n",
    "                \n",
    "                if count_to_find > 0 :\n",
    "                    counter1 = 0\n",
    "                    RL = lambd\n",
    "                    LL = 0\n",
    "                    to_find = to_find + 1\n",
    "\n",
    "                elif counter1 >= search_limit:\n",
    "                    counter1 = 0\n",
    "                    RL = lambd/2\n",
    "                    LL = 0\n",
    "                    to_find = to_find + 1\n",
    "\n",
    "\n",
    "                elif nonzero < to_find:\n",
    "                    RL = lambd\n",
    "                    counter1 = counter1 + 1 \n",
    "\n",
    "                elif nonzero >= to_find:\n",
    "                    LL = lambd\n",
    "                    counter1 = counter1 + 1 \n",
    "                    \n",
    "            elif arg[9] == 'constrained':\n",
    "                \n",
    "                if count_to_find > 0 :\n",
    "                    counter1 = 0\n",
    "                    RL = lambd \n",
    "                    LL = 0\n",
    "                    to_find = to_find + 1\n",
    "\n",
    "                elif counter1 >= search_limit:\n",
    "                    counter1 = 0\n",
    "                    RL = l_start\n",
    "                    LL = 0\n",
    "                    to_find = to_find + 1\n",
    "        \n",
    "                elif nonzero > to_find:\n",
    "                    RL = lambd\n",
    "                    counter1 = counter1 + 1 \n",
    "\n",
    "                elif nonzero <= to_find:\n",
    "                    LL = lambd\n",
    "                    counter1 = counter1 + 1 \n",
    "              \n",
    "        trial = trial + 1   \n",
    "    return test_error_result,nonzero_result,train_error_result\n",
    "\n",
    "def baseline(xTrain,yTrain,xTest,yTest,problem_type,range1):\n",
    "    \n",
    "    if problem_type == 'Regression':\n",
    "        model = RandomForestRegressor(n_estimators = 100)\n",
    "        base = 1\n",
    "    if problem_type == 'Classification':\n",
    "        model = RandomForestClassifier(n_estimators = 100)\n",
    "        base = 0.5    \n",
    "    \n",
    "    rf = model.fit(xTrain,yTrain)\n",
    "    imp = pd.DataFrame(np.column_stack((xTrain.columns,rf.feature_importances_)),columns = ['features','scores']).sort_values('scores',ascending = False)\n",
    "    print(imp)\n",
    "    acc =[]\n",
    "    n_features = []\n",
    "    se = []\n",
    "    for i in range1:\n",
    "\n",
    "        if i == 0:\n",
    "            acc.append(base)\n",
    "            se.append(0)\n",
    "            n_features.append(i)\n",
    "            continue\n",
    "            \n",
    "        to_use = imp.head(i)['features'].values\n",
    "        trial = 0\n",
    "        acc1 = []\n",
    "        while trial < 1:\n",
    "            rf1 = model.fit(xTrain[to_use],yTrain)\n",
    "            \n",
    "            if problem_type == 'Regression':\n",
    "                pred = rf1.predict(xTest[to_use])\n",
    "                acc1.append(np.sqrt(np.mean((yTest-pred)**2)))\n",
    "                \n",
    "            if problem_type == 'Classification':\n",
    "                pred = rf1.predict_proba(xTest[to_use])[:,1]\n",
    "                acc1.append(sklearn.metrics.roc_auc_score(yTest,pred))\n",
    "                \n",
    "            trial = trial+1\n",
    "            \n",
    "        acc.append(np.mean(acc1))\n",
    "        se.append(np.std(acc1))\n",
    "        n_features.append(i)\n",
    "    return acc,n_features,se\n",
    "\n",
    "def plot_tradeoff_curve(test_acc,nonzero,color,label):\n",
    "    results = pd.DataFrame()\n",
    "    for i in range(0,len(test_acc)):\n",
    "        results = results.append(pd.DataFrame(np.column_stack((test_acc[i],nonzero[i])),columns = ['test_acc','nonzero']))\n",
    "    agg = results.groupby(['nonzero'], as_index=False).agg({'test_acc':['mean','std','count']})\n",
    "    \n",
    "    plt.scatter(agg['nonzero'],agg['test_acc']['mean'],color = color,label = label)\n",
    "    plt.errorbar(agg['nonzero'],agg['test_acc']['mean'], agg['test_acc']['std'],color = color)\n",
    "    plt.xlabel('Number of Nonzero Features')\n",
    "    plt.ylabel('Test Error')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load PMLB Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chess\n"
     ]
    }
   ],
   "source": [
    "dataset_names = ['analcatdata_bankruptcy'\n",
    ",'analcatdata_boxing2'\n",
    ",'analcatdata_cyyoung8092'\n",
    ",'analcatdata_japansolvent'\n",
    ",'analcatdata_lawsuit'\n",
    ",'appendicitis'\n",
    ",'breast_cancer_wisconsin'\n",
    ",'bupa'\n",
    ",'diabetes'\n",
    ",'glass2'\n",
    ",'haberman'\n",
    ",'lupus'\n",
    ",'phoneme'\n",
    ",'pima'\n",
    ",'prnn_crabs'\n",
    ",'prnn_synth'\n",
    ",'ring'\n",
    ",'twonorm'\n",
    ",'wdbc'\n",
    ",'spectf',\n",
    "'chess'\n",
    ",'dis'\n",
    ",'horse_colic'\n",
    ",'hypothyroid'\n",
    ",'colic',\n",
    "'sonar',\n",
    "'Hill_Valley_without_noise',\n",
    "'crx','clean1','tokyo1','spambase','ionosphere','churn',\n",
    "'Hill_Valley_with_noise','analcatdata_cyyoung9302','australian','biomed',\n",
    "'buggyCrx','cleve','credit_a','heart_c','heart_h']\n",
    "\n",
    "\n",
    "dataset = 'chess'\n",
    "\n",
    "print(dataset)\n",
    "data = fetch_data(dataset)\n",
    "\n",
    "y = data['target']\n",
    "X = data.drop('target',axis = 1)\n",
    "features = X.columns\n",
    "X = preprocessing.scale(X)\n",
    "X = pd.DataFrame(X,columns = features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duplication Step for Semi Synthetic Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat</th>\n",
       "      <th>imp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>A20</td>\n",
       "      <td>0.218904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>A32</td>\n",
       "      <td>0.182961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A09</td>\n",
       "      <td>0.175894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>A14</td>\n",
       "      <td>0.0418604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>A34</td>\n",
       "      <td>0.0359373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A05</td>\n",
       "      <td>0.0351542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>A31</td>\n",
       "      <td>0.0324985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A06</td>\n",
       "      <td>0.0226181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A00</td>\n",
       "      <td>0.0193865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A07</td>\n",
       "      <td>0.0192687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>A17</td>\n",
       "      <td>0.017211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>A33</td>\n",
       "      <td>0.0165472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>A10</td>\n",
       "      <td>0.0150327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>A15</td>\n",
       "      <td>0.0146883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>A26</td>\n",
       "      <td>0.0122274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A04</td>\n",
       "      <td>0.01131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>A22</td>\n",
       "      <td>0.0108148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>A23</td>\n",
       "      <td>0.0102077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A08</td>\n",
       "      <td>0.00918867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>A12</td>\n",
       "      <td>0.00892082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>A21</td>\n",
       "      <td>0.00855952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A01</td>\n",
       "      <td>0.00854001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>A30</td>\n",
       "      <td>0.00824711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A03</td>\n",
       "      <td>0.00760776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>A28</td>\n",
       "      <td>0.00730592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>A19</td>\n",
       "      <td>0.00718908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>A25</td>\n",
       "      <td>0.00717435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>A35</td>\n",
       "      <td>0.00649596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>A16</td>\n",
       "      <td>0.00612948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>A29</td>\n",
       "      <td>0.00597633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A02</td>\n",
       "      <td>0.00547523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>A11</td>\n",
       "      <td>0.00487576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>A13</td>\n",
       "      <td>0.00277757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>A24</td>\n",
       "      <td>0.00151839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>A18</td>\n",
       "      <td>0.000789363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>A27</td>\n",
       "      <td>0.000706678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feat          imp\n",
       "20  A20     0.218904\n",
       "32  A32     0.182961\n",
       "9   A09     0.175894\n",
       "14  A14    0.0418604\n",
       "34  A34    0.0359373\n",
       "5   A05    0.0351542\n",
       "31  A31    0.0324985\n",
       "6   A06    0.0226181\n",
       "0   A00    0.0193865\n",
       "7   A07    0.0192687\n",
       "17  A17     0.017211\n",
       "33  A33    0.0165472\n",
       "10  A10    0.0150327\n",
       "15  A15    0.0146883\n",
       "26  A26    0.0122274\n",
       "4   A04      0.01131\n",
       "22  A22    0.0108148\n",
       "23  A23    0.0102077\n",
       "8   A08   0.00918867\n",
       "12  A12   0.00892082\n",
       "21  A21   0.00855952\n",
       "1   A01   0.00854001\n",
       "30  A30   0.00824711\n",
       "3   A03   0.00760776\n",
       "28  A28   0.00730592\n",
       "19  A19   0.00718908\n",
       "25  A25   0.00717435\n",
       "35  A35   0.00649596\n",
       "16  A16   0.00612948\n",
       "29  A29   0.00597633\n",
       "2   A02   0.00547523\n",
       "11  A11   0.00487576\n",
       "13  A13   0.00277757\n",
       "24  A24   0.00151839\n",
       "18  A18  0.000789363\n",
       "27  A27  0.000706678"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_duplicate = [# add number of times to duplicate]\n",
    "for col in to_duplicate:\n",
    "    for i in range(7):\n",
    "        name_col = col +'dup'+str(i)\n",
    "        X[name_col] = X[col] + np.random.normal(0,.1,len(X))\n",
    "importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Real World Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "def load_adult():\n",
    "    data = pd.read_csv(\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\",\n",
    "        header=None)\n",
    "    data.columns = [\n",
    "            \"Age\", \"WorkClass\", \"fnlwgt\", \"Education\", \"EducationNum\",\n",
    "            \"MaritalStatus\", \"Occupation\", \"Relationship\", \"Race\", \"Gender\",\n",
    "            \"CapitalGain\", \"CapitalLoss\", \"HoursPerWeek\", \"NativeCountry\", \"Income\"\n",
    "        ]\n",
    "    data['target'] = 0\n",
    "    data = data.sample(frac = 1)\n",
    "    data['target'].loc[data['Income']== data['Income'].unique()[1]] = 1\n",
    "    y = data['target']\n",
    "    data.drop(['target','Occupation','Income'],axis = 1, inplace = True)\n",
    "    data = pd.get_dummies(data, columns = ['WorkClass','Education','MaritalStatus','Relationship','Race','Gender'])\n",
    "    data['NativeCountry'] = data['NativeCountry'] == ' United-States'\n",
    "    data['NativeCountry'] = data['NativeCountry'].astype(int)\n",
    "    features = data.columns\n",
    "    X = preprocessing.scale(data)\n",
    "    X = pd.DataFrame(X,columns = features)\n",
    "    #xTrain, xTest, yTrain, yTest = train_test_split(X,y, test_size = 0.3)\n",
    "    return X,y\n",
    "\n",
    "def load_audit():\n",
    "    audit_risk = pd.read_csv(\"audit_risk.csv\")\n",
    "    trial = pd.read_csv(\"trial.csv\")\n",
    "    trial.columns = ['Sector_score','LOCATION_ID', 'PARA_A', 'Score_A', 'PARA_B',\n",
    "           'Score_B',  'TOTAL', 'numbers', 'Marks',\n",
    "           'Money_Value', 'MONEY_Marks', 'District',\n",
    "           'Loss', 'LOSS_SCORE', 'History', 'History_score', 'Score', 'Risk_trial' ]\n",
    "    trial['Score_A'] = trial['Score_A']/10\n",
    "    trial['Score_B'] = trial['Score_B']/10\n",
    "    merged_df = pd.merge(audit_risk, trial, how='outer', on = ['History', 'LOCATION_ID', 'Money_Value', 'PARA_A', 'PARA_B',\n",
    "           'Score', 'Score_A', 'Score_B', 'Sector_score', 'TOTAL', 'numbers'])\n",
    "\n",
    "    df = merged_df.drop(['Risk_trial'], axis = 1)\n",
    "    df['Money_Value'] = df['Money_Value'].fillna(df['Money_Value'].median())\n",
    "    df = df.drop(['Detection_Risk', 'Risk_F'], axis = 1) \n",
    "    df = df[(df.LOCATION_ID != 'LOHARU')]\n",
    "    df = df[(df.LOCATION_ID != 'NUH')]\n",
    "    df = df[(df.LOCATION_ID != 'SAFIDON')]\n",
    "    df = df.astype(float)\n",
    "    df = df.drop_duplicates(keep = 'first')\n",
    "    df = df.sample(frac=1)\n",
    "    class_df = df.drop([\"Audit_Risk\",'Inherent_Risk','Score','TOTAL'], axis = 1)\n",
    "    y = class_df[\"Risk\"]    \n",
    "    classification_X = class_df.drop([\"Risk\"], axis = 1)\n",
    "    cols = classification_X.columns\n",
    "    X = preprocessing.scale(classification_X)\n",
    "    X = pd.DataFrame(X,columns = cols)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth= 10\n",
    "problem_type = 'Classification'\n",
    "loss_type = 'logistic'\n",
    "optimization_type = 'penalized'\n",
    "\n",
    "\n",
    "lambd=  0.01\n",
    "threshold= 10**-3\n",
    "ntrials = 10\n",
    "features_to_find = min(len(X.columns),10)\n",
    "search_limit = 20\n",
    "l_start = 10\n",
    "\n",
    "bag_test_acc = []\n",
    "bag_nonzero = []\n",
    "base_line_acc = []\n",
    "baseline_nonzero = []\n",
    "baseline_se = []\n",
    "\n",
    "# K Fold Experiment\n",
    "kf = KFold(n_splits=4)\n",
    "kf.get_n_splits(X)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    xTrain, xTest = X.iloc[train_index], X.iloc[test_index]\n",
    "    yTrain, yTest = y.iloc[train_index], y.iloc[test_index]\n",
    "    arg = [xTrain,yTrain,xTest,yTest, max_depth,problem_type,loss_type,lambd,threshold,optimization_type]\n",
    "    bag_test_acc1,bag_nonzero1,bag_train_acc1 = run_experiment(arg,ntrials,features_to_find,search_limit,l_start,build_trees_bag)\n",
    "    bag_test_acc = np.append(bag_test_acc,bag_test_acc1)\n",
    "    bag_nonzero = np.append(bag_nonzero,bag_nonzero1)\n",
    "    \n",
    "    range1 = np.unique(bag_nonzero1)\n",
    "    base_line_acc1,baseline_nonzero1,baseline_se1 = baseline(xTrain,yTrain,xTest,yTest,problem_type,range1)\n",
    "    \n",
    "    base_line_acc = np.append(base_line_acc,base_line_acc1)\n",
    "    baseline_nonzero = np.append(baseline_nonzero,baseline_nonzero1)\n",
    "    \n",
    "baseline1 = pd.DataFrame(np.column_stack((baseline_nonzero,base_line_acc)),columns = ['nonzero','acc'])\n",
    "baseline1 = baseline1.groupby('nonzero').agg(['mean','std']).reset_index()\n",
    "baseline1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tradeoff_curve(bag_test_acc,bag_nonzero,'blue',label = 'ControlBurn')\n",
    "plt.ylabel('ROC-AUC')\n",
    "plt.xlabel('Number of Non-Zero Features')\n",
    "plt.xlim(0,10)\n",
    "plt.scatter(baseline1['nonzero'],baseline1['acc']['mean'],label = 'Random Forest',color = 'grey')\n",
    "plt.errorbar(baseline1['nonzero'],baseline1['acc']['mean'], baseline1['acc']['std'],color = 'grey')\n",
    "plt.title(dataset)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chess373duplicate.pkl\n"
     ]
    }
   ],
   "source": [
    "filename = dataset+'.pkl'\n",
    "results = {}\n",
    "results['name'] = dataset\n",
    "results['baseline_acc'] = base_line_acc\n",
    "results['baseline_nonzero'] = baseline_nonzero\n",
    "results['nonzero'] = bag_nonzero\n",
    "results['acc'] = bag_test_acc\n",
    "results['nrow'] = len(y)\n",
    "results['ncol'] = len(X.columns)\n",
    "\n",
    "with open(filename, 'wb') as handle:\n",
    "    pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
