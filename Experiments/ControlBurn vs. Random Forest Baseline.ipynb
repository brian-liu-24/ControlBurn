{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from ControlBurn.ControlBurnExperiment import run_experiment\n",
    "from ControlBurn.ControlBurnExperiment import plot_tradeoff_curve\n",
    "from ControlBurn.RandomForestBaseline import RandomForestBaseline\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "from pmlb import fetch_data\n",
    "from pmlb import classification_dataset_names, regression_dataset_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "## Load PMLB Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = ['analcatdata_bankruptcy'\n",
    ",'analcatdata_boxing2'\n",
    ",'analcatdata_cyyoung8092'\n",
    ",'analcatdata_japansolvent'\n",
    ",'analcatdata_lawsuit'\n",
    ",'appendicitis'\n",
    ",'breast_cancer_wisconsin'\n",
    ",'bupa'\n",
    ",'diabetes'\n",
    ",'glass2'\n",
    ",'haberman'\n",
    ",'lupus'\n",
    ",'phoneme'\n",
    ",'pima'\n",
    ",'prnn_crabs'\n",
    ",'prnn_synth'\n",
    ",'ring'\n",
    ",'twonorm'\n",
    ",'wdbc'\n",
    ",'spectf',\n",
    "'chess'\n",
    ",'dis'\n",
    ",'horse_colic'\n",
    ",'hypothyroid'\n",
    ",'colic',\n",
    "'sonar',\n",
    "'Hill_Valley_without_noise',\n",
    "'crx','clean1','tokyo1','spambase','ionosphere','churn',\n",
    "'Hill_Valley_with_noise','analcatdata_cyyoung9302','australian','biomed',\n",
    "'buggyCrx','cleve','credit_a','heart_c','heart_h']\n",
    "\n",
    "\n",
    "dataset = 'chess'\n",
    "\n",
    "print(dataset)\n",
    "data = fetch_data(dataset)\n",
    "\n",
    "y = data['target']\n",
    "X = data.drop('target',axis = 1)\n",
    "features = X.columns\n",
    "X = preprocessing.scale(X)\n",
    "X = pd.DataFrame(X,columns = features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplication Step for Semi Synthetic Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == 'Chess':\n",
    "    rf = RandomForestClassifier().fit(X,y)\n",
    "    importances = pd.DataFrame(np.column_stack((X.columns,rf.feature_importances_))\n",
    "                 ,columns = ['feat','imp']).sort_values('imp',ascending = False)\n",
    "    to_duplicate = importances.head(3)['feat'].values\n",
    "    for col in to_duplicate:\n",
    "        for i in range(7):\n",
    "            name_col = col +'dup'+str(i)\n",
    "            X[name_col] = X[col] + np.random.normal(0,.1,len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Real World Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "def load_adult():\n",
    "    data = pd.read_csv(\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\",\n",
    "        header=None)\n",
    "    data.columns = [\n",
    "            \"Age\", \"WorkClass\", \"fnlwgt\", \"Education\", \"EducationNum\",\n",
    "            \"MaritalStatus\", \"Occupation\", \"Relationship\", \"Race\", \"Gender\",\n",
    "            \"CapitalGain\", \"CapitalLoss\", \"HoursPerWeek\", \"NativeCountry\", \"Income\"\n",
    "        ]\n",
    "    data['target'] = 0\n",
    "    data = data.sample(frac = 1)\n",
    "    data['target'].loc[data['Income']== data['Income'].unique()[1]] = 1\n",
    "    y = data['target']\n",
    "    data.drop(['target','Occupation','Income'],axis = 1, inplace = True)\n",
    "    data = pd.get_dummies(data, columns = ['WorkClass','Education','MaritalStatus','Relationship','Race','Gender'])\n",
    "    data['NativeCountry'] = data['NativeCountry'] == ' United-States'\n",
    "    data['NativeCountry'] = data['NativeCountry'].astype(int)\n",
    "    features = data.columns\n",
    "    X = preprocessing.scale(data)\n",
    "    X = pd.DataFrame(X,columns = features)\n",
    "    #xTrain, xTest, yTrain, yTest = train_test_split(X,y, test_size = 0.3)\n",
    "    return X,y\n",
    "\n",
    "def load_audit():\n",
    "    audit_risk = pd.read_csv(\"../Data/audit_risk.csv\")\n",
    "    trial = pd.read_csv(\"trial.csv\")\n",
    "    trial.columns = ['Sector_score','LOCATION_ID', 'PARA_A', 'Score_A', 'PARA_B',\n",
    "           'Score_B',  'TOTAL', 'numbers', 'Marks',\n",
    "           'Money_Value', 'MONEY_Marks', 'District',\n",
    "           'Loss', 'LOSS_SCORE', 'History', 'History_score', 'Score', 'Risk_trial' ]\n",
    "    trial['Score_A'] = trial['Score_A']/10\n",
    "    trial['Score_B'] = trial['Score_B']/10\n",
    "    merged_df = pd.merge(audit_risk, trial, how='outer', on = ['History', 'LOCATION_ID', 'Money_Value', 'PARA_A', 'PARA_B',\n",
    "           'Score', 'Score_A', 'Score_B', 'Sector_score', 'TOTAL', 'numbers'])\n",
    "\n",
    "    df = merged_df.drop(['Risk_trial'], axis = 1)\n",
    "    df['Money_Value'] = df['Money_Value'].fillna(df['Money_Value'].median())\n",
    "    df = df.drop(['Detection_Risk', 'Risk_F'], axis = 1) \n",
    "    df = df[(df.LOCATION_ID != 'LOHARU')]\n",
    "    df = df[(df.LOCATION_ID != 'NUH')]\n",
    "    df = df[(df.LOCATION_ID != 'SAFIDON')]\n",
    "    df = df.astype(float)\n",
    "    df = df.drop_duplicates(keep = 'first')\n",
    "    df = df.sample(frac=1)\n",
    "    class_df = df.drop([\"Audit_Risk\",'Inherent_Risk','Score','TOTAL'], axis = 1)\n",
    "    y = class_df[\"Risk\"]    \n",
    "    classification_X = class_df.drop([\"Risk\"], axis = 1)\n",
    "    cols = classification_X.columns\n",
    "    X = preprocessing.scale(classification_X)\n",
    "    X = pd.DataFrame(X,columns = cols)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth= 10\n",
    "problem_type = 'Classification'\n",
    "loss_type = 'logistic'\n",
    "optimization_type = 'penalized'\n",
    "\n",
    "\n",
    "lambd=  0.01\n",
    "threshold= 10**-3\n",
    "ntrials = 10\n",
    "features_to_find = min(len(X.columns),10)\n",
    "search_limit = 20\n",
    "l_start = 10\n",
    "\n",
    "bag_test_acc = []\n",
    "bag_nonzero = []\n",
    "base_line_acc = []\n",
    "baseline_nonzero = []\n",
    "baseline_se = []\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=4)\n",
    "kf.get_n_splits(X)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    xTrain, xTest = X.iloc[train_index], X.iloc[test_index]\n",
    "    yTrain, yTest = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    arg = [xTrain,yTrain,xTest,yTest, max_depth,problem_type,loss_type,lambd,threshold,optimization_type]\n",
    "    bag_test_acc1,bag_nonzero1,bag_train_acc1 = run_experiment(arg,ntrials,features_to_find,search_limit,l_start)\n",
    "    bag_test_acc = np.append(bag_test_acc,bag_test_acc1)\n",
    "    bag_nonzero = np.append(bag_nonzero,bag_nonzero1)\n",
    "    \n",
    "    range1 = np.unique(bag_nonzero1)\n",
    "    base_line_acc1,baseline_nonzero1,baseline_se1 = RandomForestBaseline(xTrain,yTrain,xTest,yTest,problem_type,range1)\n",
    "    \n",
    "    base_line_acc = np.append(base_line_acc,base_line_acc1)\n",
    "    baseline_nonzero = np.append(baseline_nonzero,baseline_nonzero1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tradeoff_curve(bag_test_acc,bag_nonzero,'blue',label = 'ControlBurn')\n",
    "plt.ylabel('ROC-AUC')\n",
    "plt.xlabel('Number of Non-Zero Features')\n",
    "plt.xlim(0,10)\n",
    "plt.scatter(baseline1['nonzero'],baseline1['acc']['mean'],label = 'Random Forest',color = 'grey')\n",
    "plt.errorbar(baseline1['nonzero'],baseline1['acc']['mean'], baseline1['acc']['std'],color = 'grey')\n",
    "plt.title(dataset)\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
